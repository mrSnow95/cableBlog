<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.8.0 by Michael Rose
  Copyright 2017 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE.txt
-->
<html lang="en" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin SEO -->









<title>A Simple Neural Net - pi_squared</title>




<meta name="description" content="Neurons">




<meta name="author" content="Bruno Neves">

<meta property="og:locale" content="en">
<meta property="og:site_name" content="pi_squared">
<meta property="og:title" content="A Simple Neural Net">


  <link rel="canonical" href="http://localhost:4000/cableBlog/A-Simple-Neural-Net/">
  <meta property="og:url" content="http://localhost:4000/cableBlog/A-Simple-Neural-Net/">



  <meta property="og:description" content="Neurons">

















  

  





  <meta property="og:type" content="article">
  <meta property="article:published_time" content="2021-06-06T00:00:00+01:00">








  <script type="application/ld+json">
    {
      "@context" : "http://schema.org",
      "@type" : "Person",
      "name" : "",
      "url" : "http://localhost:4000/cableBlog",
      "sameAs" : null
    }
  </script>







<!-- end SEO -->


<link href="http://localhost:4000/cableBlog/feed.xml" type="application/atom+xml" rel="alternate" title="pi_squared Feed">

<!-- http://t.co/dKP3o1e -->
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">


<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>



<!-- For all browsers -->
<link rel="stylesheet" href="http://localhost:4000/cableBlog/assets/css/main.css">

<!--[if lte IE 9]>
  <style>
    /* old IE unsupported flexbox fixes */
    .greedy-nav .site-title {
      padding-right: 3em;
    }
    .greedy-nav button {
      position: absolute;
      top: 0;
      right: 0;
      height: 100%;
    }
  </style>
<![endif]-->


    <!-- start custom head snippets -->

<!-- insert favicons. use http://realfavicongenerator.net/ -->


<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    TeX: {
      equationNumbers: {
        autoNumber: "AMS"
      }
    },
    tex2jax: {
      inlineMath: [],
      displayMath: [],
      processEscapes: false,
    }
  });
</script>
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML' async></script>



<!-- end custom head snippets -->

    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/katex.min.css" integrity="sha384-yFRtMMDnQtDRO8rLpMIKrtPCD5jdktao2TV19YiZYWMDkUR5GQZR/NOVTdquEx1j" crossorigin="anonymous">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/katex.min.js" integrity="sha384-9Nhn55MVVN0/4OFx7EE5kpFBPsEMZxKTCnA+4fqDmg12eCTqGi6+BB2LjY8brQxJ" crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous" onload="renderMathInElement(document.body);"></script>

  </head>

  <body class="layout--single">

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->

    <div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        <a class="site-title" href="http://localhost:4000/cableBlog/">pi_squared</a>
        <ul class="visible-links">
          
            
            <li class="masthead__menu-item">
              <a href="http://localhost:4000/cableBlog/about/" >About</a>
            </li>
          
            
            <li class="masthead__menu-item">
              <a href="http://localhost:4000/cableBlog/posts/" >Posts</a>
            </li>
          
        </ul>
        
        <button class="greedy-nav__toggle" type="button">
          <span class="visually-hidden">Toggle Menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>

    <div class="initial-content">
      



<div id="main" role="main">
  
  <div class="sidebar sticky">
  

<div itemscope itemtype="http://schema.org/Person">

  
    <div class="author__avatar">
      

      
        <img src="http://localhost:4000/cableBlog/assets/images/profile-1.jpg" alt="Bruno Neves" itemprop="image">
      
    </div>
  

  <div class="author__content">
    
      <h3 class="author__name" itemprop="name">Bruno Neves</h3>
    
    
      <p class="author__bio" itemprop="description">
        Computer Engineer
      </p>
    
  </div>

  <div class="author__urls-wrapper">
    <button class="btn btn--inverse">Follow</button>
    <ul class="author__urls social-icons">
      
        <li itemprop="homeLocation" itemscope itemtype="http://schema.org/Place">
          <i class="fa fa-fw fa-map-marker" aria-hidden="true"></i> <span itemprop="name">Braga, Portugal</span>
        </li>
      

      

      
        <li>
          <a href="mailto:neves.brunobr@gmail.com">
            <meta itemprop="email" content="neves.brunobr@gmail.com" />
            <i class="fa fa-fw fa-envelope-square" aria-hidden="true"></i> Email
          </a>
        </li>
      

      

      

      

      

      

      

      
        <li>
          <a href="https://instagram.com/brunosszaa" itemprop="sameAs">
            <i class="fa fa-fw fa-instagram" aria-hidden="true"></i> Instagram
          </a>
        </li>
      

      

      

      
        <li>
          <a href="https://github.com/mrSnow95" itemprop="sameAs">
            <i class="fa fa-fw fa-github" aria-hidden="true"></i> GitHub
          </a>
        </li>
      

      

      

      

      

      

      

      

      

      

      

      

      

      

      <!--
  <li>
    <a href="http://link-to-whatever-social-network.com/user/" itemprop="sameAs">
      <i class="fa fa-fw" aria-hidden="true"></i> Custom Social Profile Link
    </a>
  </li>
-->
    </ul>
  </div>
</div>

  
  </div>



  <article class="page" itemscope itemtype="http://schema.org/CreativeWork">
    <meta itemprop="headline" content="A Simple Neural Net">
    <meta itemprop="description" content="Neurons">
    <meta itemprop="datePublished" content="June 06, 2021">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 class="page__title" itemprop="headline">A Simple Neural Net
</h1>
          <p class="page__meta">
            
              <time datetime="2021-06-06T00:00:00+01:00">June 06, 2021</time> &nbsp;&nbsp;&nbsp;
            
            
              <i class="fa fa-clock-o" aria-hidden="true"></i> 




  7 minute read
 &nbsp;&nbsp;&nbsp;
            
          </p>
        </header>
      

      <section class="page__content" itemprop="text">
        
        <p><strong>Neurons</strong></p>

<p>A neuron is the basic unit of a neural network. A neuron takes inputs, does some math with them, and produces one output. Here’s what a 2-input neuron looks like:</p>

<p><img src="../assets/images/neuron.png" alt="My helpful screenshot" /></p>

<p>3 things are happening here.</p>

<p>First, each input is multiplied by a weight:</p>

\[x_{1} \longrightarrow x_{1} * w_{1} \\x_{2} \longrightarrow x_{2} * w_{2}\]

<p>Next, all the weighted inputs are added together with a bias b</p>

\[(x_{1} * w_{1}) + (x_{2} * w_{2}) + b\]

<p>Finally the result is passes to the activation function:</p>

\[y = f((x_{1} * w_{1}) + (x_{2} * w_{2}) + b)\]

<p>The activation function is used to turn an unbounded input into an output that has a nice, predictable form. A commonly used activation function is the sigmoid function:</p>

<p><img src="../assets/images/sigmoid.png" alt="My helpful screenshot" /></p>

<p>In Numpy, we have:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="k">def</span> <span class="nf">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
  <span class="c1"># Our activation function: f(x) = 1 / (1 + e^(-x))
</span>  <span class="k">return</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">))</span>

<span class="k">class</span> <span class="nc">Neuron</span><span class="p">:</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">weights</span><span class="p">,</span> <span class="n">bias</span><span class="p">):</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">weights</span> <span class="o">=</span> <span class="n">weights</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">bias</span> <span class="o">=</span> <span class="n">bias</span>

  <span class="k">def</span> <span class="nf">feedforward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
    <span class="c1"># Weight inputs, add bias, then use the activation function
</span>    <span class="n">total</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">weights</span><span class="p">,</span> <span class="n">inputs</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">bias</span>
    <span class="k">return</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">total</span><span class="p">)</span>

<span class="n">weights</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span> <span class="c1"># w1 = 0, w2 = 1
</span><span class="n">bias</span> <span class="o">=</span> <span class="mi">4</span>                   <span class="c1"># b = 4
</span><span class="n">n</span> <span class="o">=</span> <span class="n">Neuron</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="n">bias</span><span class="p">)</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>       <span class="c1"># x1 = 2, x2 = 3
</span><span class="k">print</span><span class="p">(</span><span class="n">n</span><span class="p">.</span><span class="n">feedforward</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>    <span class="c1"># 0.9990889488055994
</span>
</code></pre></div></div>

<p><strong>Neurons in a Neural Net</strong></p>

<p>A neural network is nothing more than a bunch of neurons connected together:</p>

<p><img src="../assets/images/neuralnet.png" alt="My helpful screenshot" /></p>

<p>This network has 2 inputs, a hidden layer with 2 neurons ( \(h_{1}\) and \(h_{2}\) ) and an output layer with 1 neuron ( \(\sigma_{1}\)). Notice that the inputs for \(\sigma_{1}\) are the outputs from \(h_{1}\) and \(h_{2}\) , that’s what makes this a network.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="c1"># ... code from previous section here
</span>
<span class="k">class</span> <span class="nc">OurNeuralNetwork</span><span class="p">:</span>

  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">weights</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
    <span class="n">bias</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="c1"># The Neuron class here is from the previous section
</span>    <span class="bp">self</span><span class="p">.</span><span class="n">h1</span> <span class="o">=</span> <span class="n">Neuron</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="n">bias</span><span class="p">)</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">h2</span> <span class="o">=</span> <span class="n">Neuron</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="n">bias</span><span class="p">)</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">o1</span> <span class="o">=</span> <span class="n">Neuron</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="n">bias</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">feedforward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="n">out_h1</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">h1</span><span class="p">.</span><span class="n">feedforward</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">out_h2</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">h2</span><span class="p">.</span><span class="n">feedforward</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

    <span class="c1"># The inputs for o1 are the outputs from h1 and h2
</span>    <span class="n">out_o1</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">o1</span><span class="p">.</span><span class="n">feedforward</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span><span class="n">out_h1</span><span class="p">,</span> <span class="n">out_h2</span><span class="p">]))</span>

    <span class="k">return</span> <span class="n">out_o1</span>

<span class="n">network</span> <span class="o">=</span> <span class="n">OurNeuralNetwork</span><span class="p">()</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
<span class="k">print</span><span class="p">(</span><span class="n">network</span><span class="p">.</span><span class="n">feedforward</span><span class="p">(</span><span class="n">x</span><span class="p">))</span> <span class="c1"># 0.7216325609518421
</span>
</code></pre></div></div>

<p><strong>Training</strong></p>

<p>Say we have the following measurements:</p>

<p><img src="../assets/images/measures.png" alt="My helpful screenshot" /></p>

<p>Let’s train our network to predict someone’s gender given their weight and height:</p>

<p><img src="../assets/images/neuralnet2.png" alt="My helpful screenshot" /></p>

<p>Let \(0\) be a male and \(1\) a female.</p>

<p><strong>Loss</strong></p>

<p>Before we train our network, we first need a way to quantify how “good” it’s doing so that it can try to do “better”. That’s what the loss is :</p>

\[MSE = \frac{1}{n} \sum_{i=1}^{n} (y_{true} - y_{pred}) ^ 2\]

<p>MSE stands for mean squared error.</p>

<ul>
  <li>\(n\) is the number of samples, which is 4 (Alice, Bob, Charlie, Diana).</li>
  <li>\(y\) represents the variable being predicted, which is Gender</li>
  <li>\(y_{true}\) is the true value of the variable (the “correct answer”). For example, 
  \(y_{true}\) for Alice would be 1.</li>
  <li>\(y_{pred}\) is the predicted value of the variable. It’s whatever our network outputs.</li>
</ul>

<p>\((y_{true} - y_{pred}) ^ 2\) is the squared error. Our loss function is simply taking the average over all squared errors (hence the name mean squared error). The better our predictions are, the lower our loss will be!</p>

<p>In Numpy:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="k">def</span> <span class="nf">mse_loss</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">):</span>
  <span class="c1"># y_true and y_pred are numpy arrays of the same length.
</span>  <span class="k">return</span> <span class="p">((</span><span class="n">y_true</span> <span class="o">-</span> <span class="n">y_pred</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">).</span><span class="n">mean</span><span class="p">()</span>

<span class="n">y_true</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>

<span class="k">print</span><span class="p">(</span><span class="n">mse_loss</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">))</span> <span class="c1"># 0.5
</span>
</code></pre></div></div>
<p>Let’s label each weight and bias in our network:</p>

<p><img src="../assets/images/neurallabel.png" alt="My helpful screenshot" /></p>

<p>We can therefore write the Loss Function as:</p>

\[L(w_{1},w_{2},w_{3},w_{4},w_{5},w_{6},b_{1},b_{2})\]

<p>What we want is, for every little tweak in each variable, we want the minimal loss possible. We can calculate variation of the loss function regarding the weights by using the chain rule:</p>

\[\frac{\partial L }{\partial w_{i}} = \frac{\partial L }{\partial y_{pred}} * \frac{\partial y_{pred} }{\partial h_{i} } * \frac{\partial h_{i} }{\partial w_{i}}\]

<p>For our problem, lets take just the femaels predictions. We have :</p>

\[\frac{\partial L }{\partial y_{pred}} = \frac{\partial (y_{true} - y_{pred}) ^ 2  }{\partial y_{pred}}  =  -2 * (1 - y_{pred})\]

\[\frac{\partial y_{pred} }{\partial h_{1} } = w_{5} * f^{\prime}(w_{5} *h_{1} + w_{6} * h_{2} + b_{3})\]

<p>Finally,</p>

\[h_{1} = f(w_{1} * x_{1} + w_{2} * x_{2} + b_{1})\]

\[\frac{\partial h_{1} }{\partial w_{1}} = x1 * f^{\prime}(w_{1} * x_{1} + w_{2} * x_{2} + b_{1})\]

<p>For the Sigmoid function :</p>

\[f^{\prime} = f(x) * (1 - f(x))\]

<p>And thats it ! This backwards calculation of the chain rule is also known as <strong>backpropagation</strong>.</p>

<p>So, for every prediction, we can create a <strong>series of prediciton</strong>. Based on the last predicion, we can get better and better prediction until our Loss Function is small enough. For such iteration, the weight is updated as the following:</p>

\[w_{1} \longleftarrow w_{1} - \theta * \frac{\partial L}{\partial w_{1}}\]

<p>This makes sense because \(\frac{\partial L}{\partial w_{1}}\) is the direction of maximal variation o \(L\) regarding \(w_{1}\) , so taking the minus sign, we aiming towards the minimum. The \(\theta\) parameter is known as the <strong>learning rate</strong>, and dictates how fast we train.</p>

<p>And therefore, we have the final code:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="k">def</span> <span class="nf">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
  <span class="c1"># Sigmoid activation function: f(x) = 1 / (1 + e^(-x))
</span>  <span class="k">return</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">deriv_sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
  <span class="c1"># Derivative of sigmoid: f'(x) = f(x) * (1 - f(x))
</span>  <span class="n">fx</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">fx</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">fx</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">mse_loss</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">):</span>
  <span class="c1"># y_true and y_pred are numpy arrays of the same length.
</span>  <span class="k">return</span> <span class="p">((</span><span class="n">y_true</span> <span class="o">-</span> <span class="n">y_pred</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">).</span><span class="n">mean</span><span class="p">()</span>

<span class="k">class</span> <span class="nc">OurNeuralNetwork</span><span class="p">:</span>
  
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="c1"># Weights
</span>    <span class="bp">self</span><span class="p">.</span><span class="n">w1</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">normal</span><span class="p">()</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">w2</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">normal</span><span class="p">()</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">w3</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">normal</span><span class="p">()</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">w4</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">normal</span><span class="p">()</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">w5</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">normal</span><span class="p">()</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">w6</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">normal</span><span class="p">()</span>

    <span class="c1"># Biases
</span>    <span class="bp">self</span><span class="p">.</span><span class="n">b1</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">normal</span><span class="p">()</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">b2</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">normal</span><span class="p">()</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">b3</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">normal</span><span class="p">()</span>

  <span class="k">def</span> <span class="nf">feedforward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="c1"># x is a numpy array with 2 elements.
</span>    <span class="n">h1</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">w1</span> <span class="o">*</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">w2</span> <span class="o">*</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">b1</span><span class="p">)</span>
    <span class="n">h2</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">w3</span> <span class="o">*</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">w4</span> <span class="o">*</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">b2</span><span class="p">)</span>
    <span class="n">o1</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">w5</span> <span class="o">*</span> <span class="n">h1</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">w6</span> <span class="o">*</span> <span class="n">h2</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">b3</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">o1</span>

  <span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">all_y_trues</span><span class="p">):</span>
    <span class="s">'''
    - data is a (n x 2) numpy array, n = # of samples in the dataset.
    - all_y_trues is a numpy array with n elements.
      Elements in all_y_trues correspond to those in data.
    '''</span>
    <span class="n">learn_rate</span> <span class="o">=</span> <span class="mf">0.1</span>
    <span class="n">epochs</span> <span class="o">=</span> <span class="mi">1000</span> <span class="c1"># number of times to loop through the entire dataset
</span>
    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
      <span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">y_true</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">all_y_trues</span><span class="p">):</span>
        <span class="c1"># --- Do a feedforward (we'll need these values later)
</span>        <span class="n">sum_h1</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">w1</span> <span class="o">*</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">w2</span> <span class="o">*</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">b1</span>
        <span class="n">h1</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">sum_h1</span><span class="p">)</span>

        <span class="n">sum_h2</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">w3</span> <span class="o">*</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">w4</span> <span class="o">*</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">b2</span>
        <span class="n">h2</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">sum_h2</span><span class="p">)</span>

        <span class="n">sum_o1</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">w5</span> <span class="o">*</span> <span class="n">h1</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">w6</span> <span class="o">*</span> <span class="n">h2</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">b3</span>
        <span class="n">o1</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">sum_o1</span><span class="p">)</span>
        <span class="n">y_pred</span> <span class="o">=</span> <span class="n">o1</span>

        <span class="c1"># --- Calculate partial derivatives.
</span>        <span class="c1"># --- Naming: d_L_d_w1 represents "partial L / partial w1"
</span>        <span class="n">d_L_d_ypred</span> <span class="o">=</span> <span class="o">-</span><span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="n">y_true</span> <span class="o">-</span> <span class="n">y_pred</span><span class="p">)</span>

        <span class="c1"># Neuron o1
</span>        <span class="n">d_ypred_d_w5</span> <span class="o">=</span> <span class="n">h1</span> <span class="o">*</span> <span class="n">deriv_sigmoid</span><span class="p">(</span><span class="n">sum_o1</span><span class="p">)</span>
        <span class="n">d_ypred_d_w6</span> <span class="o">=</span> <span class="n">h2</span> <span class="o">*</span> <span class="n">deriv_sigmoid</span><span class="p">(</span><span class="n">sum_o1</span><span class="p">)</span>
        <span class="n">d_ypred_d_b3</span> <span class="o">=</span> <span class="n">deriv_sigmoid</span><span class="p">(</span><span class="n">sum_o1</span><span class="p">)</span>

        <span class="n">d_ypred_d_h1</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">w5</span> <span class="o">*</span> <span class="n">deriv_sigmoid</span><span class="p">(</span><span class="n">sum_o1</span><span class="p">)</span>
        <span class="n">d_ypred_d_h2</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">w6</span> <span class="o">*</span> <span class="n">deriv_sigmoid</span><span class="p">(</span><span class="n">sum_o1</span><span class="p">)</span>

        <span class="c1"># Neuron h1
</span>        <span class="n">d_h1_d_w1</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">deriv_sigmoid</span><span class="p">(</span><span class="n">sum_h1</span><span class="p">)</span>
        <span class="n">d_h1_d_w2</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">deriv_sigmoid</span><span class="p">(</span><span class="n">sum_h1</span><span class="p">)</span>
        <span class="n">d_h1_d_b1</span> <span class="o">=</span> <span class="n">deriv_sigmoid</span><span class="p">(</span><span class="n">sum_h1</span><span class="p">)</span>

        <span class="c1"># Neuron h2
</span>        <span class="n">d_h2_d_w3</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">deriv_sigmoid</span><span class="p">(</span><span class="n">sum_h2</span><span class="p">)</span>
        <span class="n">d_h2_d_w4</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">deriv_sigmoid</span><span class="p">(</span><span class="n">sum_h2</span><span class="p">)</span>
        <span class="n">d_h2_d_b2</span> <span class="o">=</span> <span class="n">deriv_sigmoid</span><span class="p">(</span><span class="n">sum_h2</span><span class="p">)</span>

        <span class="c1"># --- Update weights and biases
</span>        <span class="c1"># Neuron h1
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">w1</span> <span class="o">-=</span> <span class="n">learn_rate</span> <span class="o">*</span> <span class="n">d_L_d_ypred</span> <span class="o">*</span> <span class="n">d_ypred_d_h1</span> <span class="o">*</span> <span class="n">d_h1_d_w1</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">w2</span> <span class="o">-=</span> <span class="n">learn_rate</span> <span class="o">*</span> <span class="n">d_L_d_ypred</span> <span class="o">*</span> <span class="n">d_ypred_d_h1</span> <span class="o">*</span> <span class="n">d_h1_d_w2</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">b1</span> <span class="o">-=</span> <span class="n">learn_rate</span> <span class="o">*</span> <span class="n">d_L_d_ypred</span> <span class="o">*</span> <span class="n">d_ypred_d_h1</span> <span class="o">*</span> <span class="n">d_h1_d_b1</span>

        <span class="c1"># Neuron h2
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">w3</span> <span class="o">-=</span> <span class="n">learn_rate</span> <span class="o">*</span> <span class="n">d_L_d_ypred</span> <span class="o">*</span> <span class="n">d_ypred_d_h2</span> <span class="o">*</span> <span class="n">d_h2_d_w3</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">w4</span> <span class="o">-=</span> <span class="n">learn_rate</span> <span class="o">*</span> <span class="n">d_L_d_ypred</span> <span class="o">*</span> <span class="n">d_ypred_d_h2</span> <span class="o">*</span> <span class="n">d_h2_d_w4</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">b2</span> <span class="o">-=</span> <span class="n">learn_rate</span> <span class="o">*</span> <span class="n">d_L_d_ypred</span> <span class="o">*</span> <span class="n">d_ypred_d_h2</span> <span class="o">*</span> <span class="n">d_h2_d_b2</span>

        <span class="c1"># Neuron o1
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">w5</span> <span class="o">-=</span> <span class="n">learn_rate</span> <span class="o">*</span> <span class="n">d_L_d_ypred</span> <span class="o">*</span> <span class="n">d_ypred_d_w5</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">w6</span> <span class="o">-=</span> <span class="n">learn_rate</span> <span class="o">*</span> <span class="n">d_L_d_ypred</span> <span class="o">*</span> <span class="n">d_ypred_d_w6</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">b3</span> <span class="o">-=</span> <span class="n">learn_rate</span> <span class="o">*</span> <span class="n">d_L_d_ypred</span> <span class="o">*</span> <span class="n">d_ypred_d_b3</span>

      <span class="c1"># --- Calculate total loss at the end of each epoch
</span>      <span class="k">if</span> <span class="n">epoch</span> <span class="o">%</span> <span class="mi">10</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">y_preds</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">apply_along_axis</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">feedforward</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">data</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">mse_loss</span><span class="p">(</span><span class="n">all_y_trues</span><span class="p">,</span> <span class="n">y_preds</span><span class="p">)</span>
        <span class="k">print</span><span class="p">(</span><span class="s">"Epoch %d loss: %.3f"</span> <span class="o">%</span> <span class="p">(</span><span class="n">epoch</span><span class="p">,</span> <span class="n">loss</span><span class="p">))</span>

<span class="c1"># Define dataset
</span><span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span>
  <span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span>  <span class="c1"># Alice
</span>  <span class="p">[</span><span class="mi">25</span><span class="p">,</span> <span class="mi">6</span><span class="p">],</span>   <span class="c1"># Bob
</span>  <span class="p">[</span><span class="mi">17</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span>   <span class="c1"># Charlie
</span>  <span class="p">[</span><span class="o">-</span><span class="mi">15</span><span class="p">,</span> <span class="o">-</span><span class="mi">6</span><span class="p">],</span> <span class="c1"># Diana
</span><span class="p">])</span>
<span class="n">all_y_trues</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span>
  <span class="mi">1</span><span class="p">,</span> <span class="c1"># Alice
</span>  <span class="mi">0</span><span class="p">,</span> <span class="c1"># Bob
</span>  <span class="mi">0</span><span class="p">,</span> <span class="c1"># Charlie
</span>  <span class="mi">1</span><span class="p">,</span> <span class="c1"># Diana
</span><span class="p">])</span>

<span class="c1"># Train our neural network!
</span><span class="n">network</span> <span class="o">=</span> <span class="n">OurNeuralNetwork</span><span class="p">()</span>
<span class="n">network</span><span class="p">.</span><span class="n">train</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">all_y_trues</span><span class="p">)</span>

</code></pre></div></div>

<p>And we have the following plot:</p>

<p><img src="../assets/images/training.png" alt="My helpful screenshot" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="c1"># Make some predictions
</span><span class="n">emily</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mi">7</span><span class="p">,</span> <span class="o">-</span><span class="mi">3</span><span class="p">])</span> <span class="c1"># 128 pounds, 63 inches
</span><span class="n">frank</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span><span class="mi">20</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>  <span class="c1"># 155 pounds, 68 inches
</span><span class="k">print</span><span class="p">(</span><span class="s">"Emily: %.3f"</span> <span class="o">%</span> <span class="n">network</span><span class="p">.</span><span class="n">feedforward</span><span class="p">(</span><span class="n">emily</span><span class="p">))</span> <span class="c1"># 0.951 - F
</span><span class="k">print</span><span class="p">(</span><span class="s">"Frank: %.3f"</span> <span class="o">%</span> <span class="n">network</span><span class="p">.</span><span class="n">feedforward</span><span class="p">(</span><span class="n">frank</span><span class="p">))</span> <span class="c1"># 0.039 - M
</span></code></pre></div></div>

        
      </section>

      <footer class="page__meta">
        
        


        
          <p class="page__date"><strong><i class="fa fa-fw fa-calendar" aria-hidden="true"></i> Updated:</strong> <time datetime="2021-06-06T00:00:00+01:00">June 06, 2021</time></p>
        
      </footer>

      <section class="page__share">
  
    <h4 class="page__share-title">Share on</h4>
  

  <a href="https://twitter.com/intent/tweet?text=A+Simple+Neural+Net%20http%3A%2F%2Flocalhost%3A4000%2FcableBlog%2FA-Simple-Neural-Net%2F" class="btn btn--twitter" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Twitter"><i class="fa fa-fw fa-twitter" aria-hidden="true"></i><span> Twitter</span></a>

  <a href="https://www.facebook.com/sharer/sharer.php?u=http%3A%2F%2Flocalhost%3A4000%2FcableBlog%2FA-Simple-Neural-Net%2F" class="btn btn--facebook" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Facebook"><i class="fa fa-fw fa-facebook" aria-hidden="true"></i><span> Facebook</span></a>

  <a href="https://plus.google.com/share?url=http%3A%2F%2Flocalhost%3A4000%2FcableBlog%2FA-Simple-Neural-Net%2F" class="btn btn--google-plus" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Google Plus"><i class="fa fa-fw fa-google-plus" aria-hidden="true"></i><span> Google+</span></a>

  <a href="https://www.linkedin.com/shareArticle?mini=true&url=http%3A%2F%2Flocalhost%3A4000%2FcableBlog%2FA-Simple-Neural-Net%2F" class="btn btn--linkedin" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on LinkedIn"><i class="fa fa-fw fa-linkedin" aria-hidden="true"></i><span> LinkedIn</span></a>
</section>


      
  <nav class="pagination">
    
      <a href="http://localhost:4000/cableBlog/Scalibility-and-Architectural-Patterns/" class="pagination--pager" title="Scalability and Architectural Patterns in Node.js
">Previous</a>
    
    
      <a href="http://localhost:4000/cableBlog/Async-Initialization-in-Node.js/" class="pagination--pager" title="Async Initialization in Node.js
">Next</a>
    
  </nav>

    </div>

    
  </article>

  
  
    <div class="page__related">
      <h4 class="page__related-title">You May Also Enjoy</h4>
      <div class="grid__wrapper">
        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="http://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="http://localhost:4000/cableBlog/Multivariable-Calculus/" rel="permalink">Multivariable Calculus
</a>
      
    </h2>
    <p class="page__meta">
      
        <time datetime="2021-07-01T00:00:00+01:00">July 01, 2021</time> &nbsp;&nbsp;&nbsp;
      
      
        <i class="fa fa-clock-o" aria-hidden="true"></i> 




  1 minute read
 &nbsp;&nbsp;&nbsp;
      
    </p>
    <p class="archive__item-excerpt" itemprop="description">For multivariable integrals, the true story starts with a change of coordinates.

</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="http://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="http://localhost:4000/cableBlog/Async-Initialization-in-Node.js/" rel="permalink">Async Initialization in Node.js
</a>
      
    </h2>
    <p class="page__meta">
      
        <time datetime="2021-06-30T00:00:00+01:00">June 30, 2021</time> &nbsp;&nbsp;&nbsp;
      
      
        <i class="fa fa-clock-o" aria-hidden="true"></i> 




  8 minute read
 &nbsp;&nbsp;&nbsp;
      
    </p>
    <p class="archive__item-excerpt" itemprop="description">Tasks that are trivial in traditional synchronous programming can become more complicated when applied to asynchronous programming. A typical example is tryi...</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="http://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="http://localhost:4000/cableBlog/Scalibility-and-Architectural-Patterns/" rel="permalink">Scalability and Architectural Patterns in Node.js
</a>
      
    </h2>
    <p class="page__meta">
      
        <time datetime="2021-06-01T00:00:00+01:00">June 01, 2021</time> &nbsp;&nbsp;&nbsp;
      
      
        <i class="fa fa-clock-o" aria-hidden="true"></i> 




  5 minute read
 &nbsp;&nbsp;&nbsp;
      
    </p>
    <p class="archive__item-excerpt" itemprop="description">Node.js is a non-blocking single thread architecture that works great for applications handling a moderate number of requests per second (usually a few hundr...</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="http://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="http://localhost:4000/cableBlog/CPU-Bound/" rel="permalink">CPU Bound
</a>
      
    </h2>
    <p class="page__meta">
      
        <time datetime="2021-06-01T00:00:00+01:00">June 01, 2021</time> &nbsp;&nbsp;&nbsp;
      
      
        <i class="fa fa-clock-o" aria-hidden="true"></i> 




  less than 1 minute read
 &nbsp;&nbsp;&nbsp;
      
    </p>
    <p class="archive__item-excerpt" itemprop="description">
</p>
  </article>
</div>

        
      </div>
    </div>
  
  
</div>

    </div>

    

    <div class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    
      <li><strong>Follow:</strong></li>
    
    
    
    
      <li><a href="https://github.com/mrSnow95"><i class="fa fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
    
    
    
    <li><a href="http://localhost:4000/cableBlog/feed.xml"><i class="fa fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
  </ul>
</div>

<div class="page__footer-copyright">&copy; 2021 pi_squared. Powered by <a href="https://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.</div>

      </footer>
    </div>

    
  <script src="http://localhost:4000/cableBlog/assets/js/main.min.js"></script>







<

  </body>
</html>